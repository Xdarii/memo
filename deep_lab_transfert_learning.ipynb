{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "deep_lab_transfert_learning",
      "provenance": [],
      "authorship_tag": "ABX9TyN5ISSjq+7o2DB7pvmQ+4LW",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Xdarii/memo/blob/main/deep_lab_transfert_learning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XBOY0fPFpxB-"
      },
      "source": [
        "from glob import glob\n",
        "import cv2\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from PIL import Image\n",
        "import scipy\n",
        "from scipy import misc\n",
        "import json\n",
        "import matplotlib.pyplot as plt\n",
        "from google.colab import drive\n",
        "import numpy as np\n",
        "import os\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zKZm7e-EskOy",
        "outputId": "304cd50d-fc18-4c28-bc7a-e34d6b5eaae6"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "import os"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sJ9pom6rqnaQ"
      },
      "source": [
        "# Loading dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fxmp-0uuqThg"
      },
      "source": [
        "from pathlib import Path\n",
        "from typing import Any, Callable, Optional\n",
        "\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "from torchvision.datasets.vision import VisionDataset\n",
        "\n",
        "\n",
        "class SegmentationDataset(VisionDataset):\n",
        "    \"\"\"A PyTorch dataset for image segmentation task.\n",
        "    The dataset is compatible with torchvision transforms.\n",
        "    The transforms passed would be applied to both the Images and Masks.\n",
        "    \"\"\"\n",
        "    def __init__(self,\n",
        "                 root: str,\n",
        "                 image_folder: str,\n",
        "                 mask_folder: str,\n",
        "                 transforms: Optional[Callable] = None,\n",
        "                 seed: int = None,\n",
        "                 fraction: float = None,\n",
        "                 subset: str = None,\n",
        "                 image_color_mode: str = \"rgb\",\n",
        "                 mask_color_mode: str = \"grayscale\") -> None:\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            root (str): Root directory path.\n",
        "            image_folder (str): Name of the folder that contains the images in the root directory.\n",
        "            mask_folder (str): Name of the folder that contains the masks in the root directory.\n",
        "            transforms (Optional[Callable], optional): A function/transform that takes in\n",
        "            a sample and returns a transformed version.\n",
        "            E.g, ``transforms.ToTensor`` for images. Defaults to None.\n",
        "            seed (int, optional): Specify a seed for the train and test split for reproducible results. Defaults to None.\n",
        "            fraction (float, optional): A float value from 0 to 1 which specifies the validation split fraction. Defaults to None.\n",
        "            subset (str, optional): 'Train' or 'Test' to select the appropriate set. Defaults to None.\n",
        "            image_color_mode (str, optional): 'rgb' or 'grayscale'. Defaults to 'rgb'.\n",
        "            mask_color_mode (str, optional): 'rgb' or 'grayscale'. Defaults to 'grayscale'.\n",
        "        Raises:\n",
        "            OSError: If image folder doesn't exist in root.\n",
        "            OSError: If mask folder doesn't exist in root.\n",
        "            ValueError: If subset is not either 'Train' or 'Test'\n",
        "            ValueError: If image_color_mode and mask_color_mode are either 'rgb' or 'grayscale'\n",
        "        \"\"\"\n",
        "        super().__init__(root, transforms)\n",
        "        image_folder_path = Path(self.root) / image_folder\n",
        "        mask_folder_path = Path(self.root) / mask_folder\n",
        "        if not image_folder_path.exists():\n",
        "            raise OSError(f\"{image_folder_path} does not exist.\")\n",
        "        if not mask_folder_path.exists():\n",
        "            raise OSError(f\"{mask_folder_path} does not exist.\")\n",
        "\n",
        "        if image_color_mode not in [\"rgb\", \"grayscale\"]:\n",
        "            raise ValueError(\n",
        "                f\"{image_color_mode} is an invalid choice. Please enter from rgb grayscale.\"\n",
        "            )\n",
        "        if mask_color_mode not in [\"rgb\", \"grayscale\"]:\n",
        "            raise ValueError(\n",
        "                f\"{mask_color_mode} is an invalid choice. Please enter from rgb grayscale.\"\n",
        "            )\n",
        "\n",
        "        self.image_color_mode = image_color_mode\n",
        "        self.mask_color_mode = mask_color_mode\n",
        "\n",
        "        if not fraction:\n",
        "            self.image_names = sorted(image_folder_path.glob(\"*\"))\n",
        "            self.mask_names = sorted(mask_folder_path.glob(\"*\"))\n",
        "        else:\n",
        "            if subset not in [\"Train\", \"Test\"]:\n",
        "                raise (ValueError(\n",
        "                    f\"{subset} is not a valid input. Acceptable values are Train and Test.\"\n",
        "                ))\n",
        "            self.fraction = fraction\n",
        "            self.image_list = np.array(sorted(image_folder_path.glob(\"*\")))\n",
        "            self.mask_list = np.array(sorted(mask_folder_path.glob(\"*\")))\n",
        "            if seed:\n",
        "                np.random.seed(seed)\n",
        "                indices = np.arange(len(self.image_list))\n",
        "                np.random.shuffle(indices)\n",
        "                self.image_list = self.image_list[indices]\n",
        "                self.mask_list = self.mask_list[indices]\n",
        "            if subset == \"Train\":\n",
        "                self.image_names = self.image_list[:int(\n",
        "                    np.ceil(len(self.image_list) * (1 - self.fraction)))]\n",
        "                self.mask_names = self.mask_list[:int(\n",
        "                    np.ceil(len(self.mask_list) * (1 - self.fraction)))]\n",
        "            else:\n",
        "                self.image_names = self.image_list[\n",
        "                    int(np.ceil(len(self.image_list) * (1 - self.fraction))):]\n",
        "                self.mask_names = self.mask_list[\n",
        "                    int(np.ceil(len(self.mask_list) * (1 - self.fraction))):]\n",
        "\n",
        "    def __len__(self) -> int:\n",
        "        return len(self.image_names)\n",
        "\n",
        "    def __getitem__(self, index: int) -> Any:\n",
        "        image_path = self.image_names[index]\n",
        "        mask_path = self.mask_names[index]\n",
        "        with open(image_path, \"rb\") as image_file, open(mask_path,\n",
        "                                                        \"rb\") as mask_file:\n",
        "            image = Image.open(image_file)\n",
        "            if self.image_color_mode == \"rgb\":\n",
        "                image = image.convert(\"RGB\").resize((224,224), Image.ANTIALIAS)\n",
        "            elif self.image_color_mode == \"grayscale\":\n",
        "                image = image.convert(\"L\").resize((224,224), Image.ANTIALIAS)\n",
        "            mask = Image.open(mask_file)\n",
        "            if self.mask_color_mode == \"rgb\":\n",
        "                mask = mask.convert(\"RGB\").resize((224,224), Image.ANTIALIAS)\n",
        "            elif self.mask_color_mode == \"grayscale\":\n",
        "                mask = mask.convert(\"L\").resize((224,224), Image.ANTIALIAS)\n",
        "                # mask = np.array(mask)            \n",
        "                # mask[mask > 0] = 1\n",
        "            sample = {\"image\": image, \"mask\": mask}\n",
        "            if self.transforms:\n",
        "                sample[\"image\"] = self.transforms(sample[\"image\"])\n",
        "                sample[\"mask\"] = self.transforms(sample[\"mask\"])\n",
        "            return sample"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HukG82Yh2XM0"
      },
      "source": [
        "## Loading and Displaying"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EA5mXiAY2jGb"
      },
      "source": [
        "Change the `**data_path**`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "miuTnbOeqZq6"
      },
      "source": [
        "from torchvision import transforms\n",
        "\n",
        "\n",
        "data_path = \n",
        "\n",
        "image_datasets = {\n",
        "    x: SegmentationDataset(data_path, \n",
        "                           \"train/image\", \n",
        "                           \"train/label\",\n",
        "                            seed=100,\n",
        "                            fraction=0.2,\n",
        "                            subset=x,\n",
        "                            transforms=transforms.Compose([transforms.ToTensor()]))  \n",
        "    for x in ['Train', 'Test']\n",
        "}\n",
        "dataloaders = {\n",
        "    x: DataLoader(image_datasets[x],\n",
        "                    batch_size=8,\n",
        "                    shuffle=True,\n",
        "                    num_workers=2)\n",
        "    for x in ['Train', 'Test']\n",
        "}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cWoELNF3tnMz"
      },
      "source": [
        "import torchvision\n",
        "\n",
        "samples = next(iter(dataloaders['Train']))\n",
        "image, labels =samples['image'],samples['mask']\n",
        "grid_labels = torchvision.utils.make_grid(labels, nrow=8)\n",
        "grid_img = torchvision.utils.make_grid(image, nrow=8)\n",
        "\n",
        "\n",
        "plt.figure(figsize=(45,45))\n",
        "plt.imshow(grid_img.permute(1, 2, 0))\n",
        "\n",
        "plt.figure(figsize=(45,45))\n",
        "plt.imshow(grid_labels.permute(1, 2, 0)*255)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E99M9QPurq9e"
      },
      "source": [
        "# Model deepLab"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s1WXaJBMDtdQ",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 66,
          "referenced_widgets": [
            "a8cacd2f10154eed9eb9951b245a738a",
            "773b99ef32694a53b5256b292267c7da",
            "e76247b28ec64f81a0a4efa113baed5b",
            "68c3ed7d79d04a1a8e076479396a008f",
            "087e34e9990042fa9be9e1d5008c7792",
            "f4fc49453b56429db8b0df5b97171370",
            "89a0a7f2c9c9437897254156872c8a5c",
            "25d0a2726619480f90331b40b28a2112",
            "cc0759eb913844598aeccf174e98dbdb",
            "6d80fc0896b540b4907f886813377fb4",
            "2d12ed68f66743448b273106a9d14017"
          ]
        },
        "outputId": "2a18c904-0f85-4660-9304-07f9d1d8e36f"
      },
      "source": [
        "\"\"\" DeepLabv3 Model download and change the head for your prediction\"\"\"\n",
        "from torchvision.models.segmentation.deeplabv3 import DeepLabHead\n",
        "from torchvision import models\n",
        "\n",
        "\n",
        "def createDeepLabv3(outputchannels=1):\n",
        "    \"\"\"DeepLabv3 class with custom head\n",
        "    Args:\n",
        "        outputchannels (int, optional): The number of output channels\n",
        "        in your dataset masks. Defaults to 1.\n",
        "    Returns:\n",
        "        model: Returns the DeepLabv3 model with the ResNet101 backbone.\n",
        "    \"\"\"\n",
        "    model = models.segmentation.deeplabv3_resnet50(pretrained=True,\n",
        "                                                    progress=True)\n",
        "    model.classifier = DeepLabHead(2048, outputchannels)\n",
        "    # Set the model in training mode\n",
        "    model.train()\n",
        "    return model\n",
        "model=createDeepLabv3(outputchannels=2)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading: \"https://download.pytorch.org/models/deeplabv3_resnet50_coco-cd0a2569.pth\" to /root/.cache/torch/hub/checkpoints/deeplabv3_resnet50_coco-cd0a2569.pth\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "a8cacd2f10154eed9eb9951b245a738a",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "  0%|          | 0.00/161M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mo2mwYoouS7f"
      },
      "source": [
        "# Metrics"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3pRHbzptuSU-"
      },
      "source": [
        "\n",
        "def pixel_accuracy(output, mask):\n",
        "    with torch.no_grad():\n",
        "        output = torch.argmax(F.softmax(output, dim=1), dim=1)\n",
        "        correct = torch.eq(output, mask).int()\n",
        "        accuracy = float(correct.sum()) / float(correct.numel())\n",
        "    return accuracy\n",
        "\n",
        "def mIoU(pred_mask, mask, smooth=1e-10, n_classes=23):\n",
        "    with torch.no_grad():\n",
        "        pred_mask = F.softmax(pred_mask, dim=1)\n",
        "        pred_mask = torch.argmax(pred_mask, dim=1)\n",
        "        pred_mask = pred_mask.contiguous().view(-1)\n",
        "        mask = mask.contiguous().view(-1)\n",
        "\n",
        "        iou_per_class = []\n",
        "        for clas in range(0, n_classes): #loop per pixel class\n",
        "            true_class = pred_mask == clas\n",
        "            true_label = mask == clas\n",
        "\n",
        "            if true_label.long().sum().item() == 0: #no exist label in this loop\n",
        "                iou_per_class.append(np.nan)\n",
        "            else:\n",
        "                intersect = torch.logical_and(true_class, true_label).sum().float().item()\n",
        "                union = torch.logical_or(true_class, true_label).sum().float().item()\n",
        "\n",
        "                iou = (intersect + smooth) / (union +smooth)\n",
        "                iou_per_class.append(iou)\n",
        "        return np.nanmean(iou_per_class)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YRDEZ8wyrvUE"
      },
      "source": [
        "# Apprentissage"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vksHm6Tc3WSs"
      },
      "source": [
        "Change the values\n",
        "\n",
        "```\n",
        "    NUM_INPUT_CHANNELS = 3\n",
        "    NUM_OUTPUT_CHANNELS = 2\n",
        "\n",
        "    NUM_EPOCHS = 100\n",
        "\n",
        "    LEARNING_RATE = 1e-3\n",
        "    MOMENTUM = 0.9\n",
        "    BATCH_SIZE = 8\n",
        "\n",
        "    save_dir=os.path.join(cdirectory,'model_weight_vege')\n",
        "\n",
        "    data_path =\n",
        "    \n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zy0Quk9I3vkx"
      },
      "source": [
        "NUM_INPUT_CHANNELS = 3\n",
        "NUM_OUTPUT_CHANNELS = 2\n",
        "\n",
        "NUM_EPOCHS = 100\n",
        "\n",
        "LEARNING_RATE = 1e-3\n",
        "MOMENTUM = 0.9\n",
        "BATCH_SIZE = 8\n",
        "\n",
        "\n",
        "save_dir=os.path.join(cdirectory,'model_weight_vege')\n",
        "\n",
        "data_path = "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3eUzBsdTrCn0"
      },
      "source": [
        "import csv\n",
        "import copy\n",
        "import time\n",
        "from tqdm import tqdm\n",
        "import torch\n",
        "import numpy as np\n",
        "import os \n",
        "from torchvision import transforms\n",
        "import torch.nn.functional as F\n",
        "def train_model(model, criterion, dataloaders, optimizer, metrics, bpath, num_epochs=3):\n",
        "    since = time.time()\n",
        "    best_model_wts = copy.deepcopy(model.state_dict())\n",
        "    best_loss = 1e10\n",
        "    # Use gpu if available\n",
        "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "    model.to(device)\n",
        "    count=0\n",
        "    # Initialize the log file for training and testing loss and metrics\n",
        "    fieldnames = ['epoch', 'Train_loss', 'Test_loss'] + \\\n",
        "        [f'Train_{m}' for m in metrics.keys()] + \\\n",
        "        [f'Test_{m}' for m in metrics.keys()]\n",
        "    with open(os.path.join(bpath, 'log.csv'), 'w', newline='') as csvfile:\n",
        "        writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
        "        writer.writeheader()\n",
        " \n",
        "    for epoch in range(1, num_epochs+1):\n",
        "        print('Epoch {}/{}'.format(epoch, num_epochs))\n",
        "        print('-' * 10)\n",
        "        # Each epoch has a training and validation phase\n",
        "        # Initialize batch summary\n",
        "        batchsummary = {a: [0] for a in fieldnames}\n",
        " \n",
        "        for phase in ['Train', 'Test']:\n",
        "          \n",
        "            if phase == 'Train':\n",
        "                model.train()  # Set model to training mode\n",
        "            else:\n",
        "                model.eval()   # Set model to evaluate mode\n",
        " \n",
        "            # Iterate over data.\n",
        "            for sample in tqdm(iter(dataloaders[phase])):\n",
        "                inputs = sample['image'].to(device)\n",
        "                masks = sample['mask'].to(device)\n",
        "                # zero the parameter gradients\n",
        "                optimizer.zero_grad()\n",
        " \n",
        "                # track history if only in train\n",
        "                with torch.set_grad_enabled(phase == 'Train'):\n",
        "\n",
        "                    outputs = model(inputs)\n",
        "                    predicted_tensor =outputs['out']\n",
        "\n",
        "                    loss = criterion(predicted_tensor, masks.to(torch.long).squeeze(1))\n",
        "                    # print('-----' * 5)\n",
        "                    for name, metric in metrics.items():\n",
        "                        if name == ' mIoU':\n",
        "                    #         # Use a classification threshold of 0.1\n",
        "                            batchsummary[f'{phase}_{name}'].append(\n",
        "                                metric(predicted_tensor, masks,n_classes=2))\n",
        "                        else:\n",
        "                            batchsummary[f'{phase}_{name}'].append(\n",
        "                                metric(predicted_tensor, masks))\n",
        " \n",
        "                    # backward + optimize only if in training phase\n",
        "                    if phase == 'Train':\n",
        "                        loss.backward()\n",
        "                        optimizer.step()\n",
        "\n",
        "            batchsummary['epoch'] = epoch\n",
        "            epoch_loss = loss\n",
        "            batchsummary[f'{phase}_loss'] = epoch_loss.item()\n",
        "            print('{} Loss: {:.4f}'.format(\n",
        "                phase, loss))\n",
        "        for field in fieldnames[3:]:\n",
        "            batchsummary[field] = np.mean(batchsummary[field])\n",
        "        print(batchsummary)\n",
        "        with open(os.path.join(bpath, 'log.csv'), 'a', newline='') as csvfile:\n",
        "            writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
        "            writer.writerow(batchsummary)\n",
        "            # deep copy the model\n",
        "            if phase == 'Test' and loss < best_loss:\n",
        "                best_loss = loss\n",
        "                best_model_wts = copy.deepcopy(model.state_dict())\n",
        "                torch.save(model.state_dict(), os.path.join(save_dir, \"model_best_vege.pth\"))\n",
        "\n",
        " \n",
        "    time_elapsed = time.time() - since\n",
        "    print('Training complete in {:.0f}m {:.0f}s'.format(\n",
        "        time_elapsed // 60, time_elapsed % 60))\n",
        "    print('Lowest Loss: {:4f}'.format(best_loss))\n",
        " \n",
        "    # load best model weights\n",
        "    model.load_state_dict(best_model_wts)\n",
        "\n",
        "    return model\n",
        "\n",
        "if __name__=='__main__':\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    if not os.path.isdir(save_dir):\n",
        "      os.makedirs(save_dir)\n",
        "\n",
        "\n",
        "    train_dataset = {\n",
        "        x: SegmentationDataset(data_path, \n",
        "                              \"train/image\", \n",
        "                              \"train/label\",\n",
        "                                seed=100,\n",
        "                                fraction=0.2,\n",
        "                                subset=x,\n",
        "                                transforms=transforms.Compose([transforms.ToTensor()]))  \n",
        "        for x in ['Train', 'Test']\n",
        "    }\n",
        "    train_dataloader = {\n",
        "        x: DataLoader(train_dataset[x],\n",
        "                        batch_size=8,\n",
        "                        shuffle=True,\n",
        "                        num_workers=2)\n",
        "        for x in ['Train', 'Test']\n",
        "    }\n",
        "    # Specify the loss function\n",
        "    # criterion = torch.nn.MSELoss(reduction='mean')\n",
        "    # Specify the optimizer with a lower learning rate\n",
        "    # optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n",
        "\n",
        "    # Specify the evaluation metrics\n",
        "    metrics = {'pixel_accuracy': pixel_accuracy, 'mIoU': mIoU}\n",
        "\n",
        " \n",
        "    # =======================================\n",
        "    # ========== if you already have a model ======================\n",
        "    # model.load_state_dict(torch.load(Path(save_dir) / \"model_best_vege.pth\",map_location=torch.device('cpu')))\n",
        "    # model.train()\n",
        "\n",
        "    criterion = torch.nn.CrossEntropyLoss()\n",
        "    optimizer = torch.optim.SGD(model.parameters(), lr=LEARNING_RATE)    \n",
        "\n",
        "    model=train_model(model, criterion, train_dataloader, optimizer, metrics, save_dir, NUM_EPOCHS)\n",
        "    torch.save(model.state_dict(), os.path.join(save_dir, \"model_best.pth\"))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xAcISdJk10DD"
      },
      "source": [
        "# Inference"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y03RF9hC1sbi"
      },
      "source": [
        "from pathlib import Path\n",
        "model = SegNet(input_channels=3,\n",
        "                        output_channels=2)\n",
        "\n",
        "model.load_state_dict(torch.load(Path(save_dir) / \"model_best_vege.pth\",map_location=torch.device('cpu')))\n",
        "model.eval()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Huh-Ebz714pN"
      },
      "source": [
        "from torchvision.utils import make_grid as makeg\n",
        "gr=makeg(masked,nrow=8)\n",
        "grid_img=makeg(d['image'])\n",
        "grid_gt=makeg(d['mask'])\n",
        "\n",
        "plt.figure(figsize=(45,45))\n",
        "plt.imshow(grid_img.permute(1, 2, 0))\n",
        "plt.figure(figsize=(45,45))\n",
        "plt.imshow(gr.permute(1, 2, 0)*255)\n",
        "plt.figure(figsize=(45,45))\n",
        "plt.imshow(grid_gt.permute(1, 2, 0)*255)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}