{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "SegNet_transfert_learning",
      "provenance": [],
      "authorship_tag": "ABX9TyNtSvVPluXIv+FKGZBi8oGU",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Xdarii/memo/blob/main/SegNet_transfert_learning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XBOY0fPFpxB-"
      },
      "source": [
        "from glob import glob\n",
        "import cv2\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from PIL import Image\n",
        "import scipy\n",
        "from scipy import misc\n",
        "import json\n",
        "import matplotlib.pyplot as plt\n",
        "from google.colab import drive\n",
        "import numpy as np\n",
        "import os\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zKZm7e-EskOy",
        "outputId": "304cd50d-fc18-4c28-bc7a-e34d6b5eaae6"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "import os"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sJ9pom6rqnaQ"
      },
      "source": [
        "# Loading dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fxmp-0uuqThg"
      },
      "source": [
        "from pathlib import Path\n",
        "from typing import Any, Callable, Optional\n",
        "\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "from torchvision.datasets.vision import VisionDataset\n",
        "\n",
        "\n",
        "class SegmentationDataset(VisionDataset):\n",
        "    \"\"\"A PyTorch dataset for image segmentation task.\n",
        "    The dataset is compatible with torchvision transforms.\n",
        "    The transforms passed would be applied to both the Images and Masks.\n",
        "    \"\"\"\n",
        "    def __init__(self,\n",
        "                 root: str,\n",
        "                 image_folder: str,\n",
        "                 mask_folder: str,\n",
        "                 transforms: Optional[Callable] = None,\n",
        "                 seed: int = None,\n",
        "                 fraction: float = None,\n",
        "                 subset: str = None,\n",
        "                 image_color_mode: str = \"rgb\",\n",
        "                 mask_color_mode: str = \"grayscale\") -> None:\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            root (str): Root directory path.\n",
        "            image_folder (str): Name of the folder that contains the images in the root directory.\n",
        "            mask_folder (str): Name of the folder that contains the masks in the root directory.\n",
        "            transforms (Optional[Callable], optional): A function/transform that takes in\n",
        "            a sample and returns a transformed version.\n",
        "            E.g, ``transforms.ToTensor`` for images. Defaults to None.\n",
        "            seed (int, optional): Specify a seed for the train and test split for reproducible results. Defaults to None.\n",
        "            fraction (float, optional): A float value from 0 to 1 which specifies the validation split fraction. Defaults to None.\n",
        "            subset (str, optional): 'Train' or 'Test' to select the appropriate set. Defaults to None.\n",
        "            image_color_mode (str, optional): 'rgb' or 'grayscale'. Defaults to 'rgb'.\n",
        "            mask_color_mode (str, optional): 'rgb' or 'grayscale'. Defaults to 'grayscale'.\n",
        "        Raises:\n",
        "            OSError: If image folder doesn't exist in root.\n",
        "            OSError: If mask folder doesn't exist in root.\n",
        "            ValueError: If subset is not either 'Train' or 'Test'\n",
        "            ValueError: If image_color_mode and mask_color_mode are either 'rgb' or 'grayscale'\n",
        "        \"\"\"\n",
        "        super().__init__(root, transforms)\n",
        "        image_folder_path = Path(self.root) / image_folder\n",
        "        mask_folder_path = Path(self.root) / mask_folder\n",
        "        if not image_folder_path.exists():\n",
        "            raise OSError(f\"{image_folder_path} does not exist.\")\n",
        "        if not mask_folder_path.exists():\n",
        "            raise OSError(f\"{mask_folder_path} does not exist.\")\n",
        "\n",
        "        if image_color_mode not in [\"rgb\", \"grayscale\"]:\n",
        "            raise ValueError(\n",
        "                f\"{image_color_mode} is an invalid choice. Please enter from rgb grayscale.\"\n",
        "            )\n",
        "        if mask_color_mode not in [\"rgb\", \"grayscale\"]:\n",
        "            raise ValueError(\n",
        "                f\"{mask_color_mode} is an invalid choice. Please enter from rgb grayscale.\"\n",
        "            )\n",
        "\n",
        "        self.image_color_mode = image_color_mode\n",
        "        self.mask_color_mode = mask_color_mode\n",
        "\n",
        "        if not fraction:\n",
        "            self.image_names = sorted(image_folder_path.glob(\"*\"))\n",
        "            self.mask_names = sorted(mask_folder_path.glob(\"*\"))\n",
        "        else:\n",
        "            if subset not in [\"Train\", \"Test\"]:\n",
        "                raise (ValueError(\n",
        "                    f\"{subset} is not a valid input. Acceptable values are Train and Test.\"\n",
        "                ))\n",
        "            self.fraction = fraction\n",
        "            self.image_list = np.array(sorted(image_folder_path.glob(\"*\")))\n",
        "            self.mask_list = np.array(sorted(mask_folder_path.glob(\"*\")))\n",
        "            if seed:\n",
        "                np.random.seed(seed)\n",
        "                indices = np.arange(len(self.image_list))\n",
        "                np.random.shuffle(indices)\n",
        "                self.image_list = self.image_list[indices]\n",
        "                self.mask_list = self.mask_list[indices]\n",
        "            if subset == \"Train\":\n",
        "                self.image_names = self.image_list[:int(\n",
        "                    np.ceil(len(self.image_list) * (1 - self.fraction)))]\n",
        "                self.mask_names = self.mask_list[:int(\n",
        "                    np.ceil(len(self.mask_list) * (1 - self.fraction)))]\n",
        "            else:\n",
        "                self.image_names = self.image_list[\n",
        "                    int(np.ceil(len(self.image_list) * (1 - self.fraction))):]\n",
        "                self.mask_names = self.mask_list[\n",
        "                    int(np.ceil(len(self.mask_list) * (1 - self.fraction))):]\n",
        "\n",
        "    def __len__(self) -> int:\n",
        "        return len(self.image_names)\n",
        "\n",
        "    def __getitem__(self, index: int) -> Any:\n",
        "        image_path = self.image_names[index]\n",
        "        mask_path = self.mask_names[index]\n",
        "        with open(image_path, \"rb\") as image_file, open(mask_path,\n",
        "                                                        \"rb\") as mask_file:\n",
        "            image = Image.open(image_file)\n",
        "            if self.image_color_mode == \"rgb\":\n",
        "                image = image.convert(\"RGB\").resize((224,224), Image.ANTIALIAS)\n",
        "            elif self.image_color_mode == \"grayscale\":\n",
        "                image = image.convert(\"L\").resize((224,224), Image.ANTIALIAS)\n",
        "            mask = Image.open(mask_file)\n",
        "            if self.mask_color_mode == \"rgb\":\n",
        "                mask = mask.convert(\"RGB\").resize((224,224), Image.ANTIALIAS)\n",
        "            elif self.mask_color_mode == \"grayscale\":\n",
        "                mask = mask.convert(\"L\").resize((224,224), Image.ANTIALIAS)\n",
        "                # mask = np.array(mask)            \n",
        "                # mask[mask > 0] = 1\n",
        "            sample = {\"image\": image, \"mask\": mask}\n",
        "            if self.transforms:\n",
        "                sample[\"image\"] = self.transforms(sample[\"image\"])\n",
        "                sample[\"mask\"] = self.transforms(sample[\"mask\"])\n",
        "            return sample"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HukG82Yh2XM0"
      },
      "source": [
        "## Loading and Displaying"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EA5mXiAY2jGb"
      },
      "source": [
        "Change the `**data_path**`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "miuTnbOeqZq6"
      },
      "source": [
        "from torchvision import transforms\n",
        " #===================\n",
        " #===================\n",
        " #===================\n",
        "\n",
        "data_path = \n",
        " #===================\n",
        " #===================\n",
        " #===================\n",
        "image_datasets = {\n",
        "    x: SegmentationDataset(data_path, \n",
        "                           \"train/image\", \n",
        "                           \"train/label\",\n",
        "                            seed=100,\n",
        "                            fraction=0.2,\n",
        "                            subset=x,\n",
        "                            transforms=transforms.Compose([transforms.ToTensor()]))  \n",
        "    for x in ['Train', 'Test']\n",
        "}\n",
        "dataloaders = {\n",
        "    x: DataLoader(image_datasets[x],\n",
        "                    batch_size=8,\n",
        "                    shuffle=True,\n",
        "                    num_workers=2)\n",
        "    for x in ['Train', 'Test']\n",
        "}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 231
        },
        "id": "cWoELNF3tnMz",
        "outputId": "05b3e51b-63c4-4bd4-d19b-96351635b8ec"
      },
      "source": [
        "import torchvision\n",
        "\n",
        "samples = next(iter(dataloaders['Train']))\n",
        "image, labels =samples['image'],samples['mask']\n",
        "grid_labels = torchvision.utils.make_grid(labels, nrow=8)\n",
        "grid_img = torchvision.utils.make_grid(image, nrow=8)\n",
        "\n",
        "from torchvision.utils import make_grid as makeg\n",
        "gr=makeg(masked,nrow=8)\n",
        "grid_img=makeg(d['image'])\n",
        "grid_gt=makeg(d['mask'])\n",
        "\n",
        "plt.figure(figsize=(45,45))\n",
        "plt.imshow(grid_img.permute(1, 2, 0))\n",
        "\n",
        "plt.figure(figsize=(45,45))\n",
        "plt.imshow(grid_labels.permute(1, 2, 0)*255)"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-2ab60e188edd>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorchvision\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0msamples\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataloaders\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Train'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mimage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0msamples\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'image'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0msamples\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'mask'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mgrid_labels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorchvision\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmake_grid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnrow\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m8\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'dataloaders' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E99M9QPurq9e"
      },
      "source": [
        "# Model SegNet"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0c_Q-6xcreyj"
      },
      "source": [
        "\"\"\"\n",
        "Pytorch implementation of SegNet (https://arxiv.org/pdf/1511.00561.pdf)\n",
        "\"\"\"\n",
        "\n",
        "from __future__ import print_function\n",
        "from collections import OrderedDict\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision.models as models\n",
        "import pprint\n",
        "\n",
        "F = nn.functional\n",
        "DEBUG = False\n",
        "\n",
        "\n",
        "vgg16_dims = [\n",
        "                    (64, 64, 'M'),                                # Stage - 1\n",
        "                    (128, 128, 'M'),                              # Stage - 2\n",
        "                    (256, 256, 256,'M'),                          # Stage - 3\n",
        "                    (512, 512, 512, 'M'),                         # Stage - 4\n",
        "                    (512, 512, 512, 'M')                          # Stage - 5\n",
        "            ]\n",
        "\n",
        "decoder_dims = [\n",
        "                    ('U', 512, 512, 512),                         # Stage - 5\n",
        "                    ('U', 512, 512, 512),                         # Stage - 4\n",
        "                    ('U', 256, 256, 256),                         # Stage - 3\n",
        "                    ('U', 128, 128),                              # Stage - 2\n",
        "                    ('U', 64, 64)                                 # Stage - 1\n",
        "                ]\n",
        "\n",
        "\n",
        "class SegNet(nn.Module):\n",
        "    def __init__(self, input_channels, output_channels):\n",
        "        super(SegNet, self).__init__()\n",
        "\n",
        "        self.input_channels = input_channels\n",
        "        self.output_channels = output_channels\n",
        "\n",
        "        self.num_channels = input_channels\n",
        "\n",
        "        self.vgg16 = models.vgg16(pretrained=True)\n",
        "\n",
        "\n",
        "        # Encoder layers\n",
        "\n",
        "        self.encoder_conv_00 = nn.Sequential(*[\n",
        "                                                nn.Conv2d(in_channels=self.input_channels,\n",
        "                                                          out_channels=64,\n",
        "                                                          kernel_size=3,\n",
        "                                                          padding=1),\n",
        "                                                nn.BatchNorm2d(64)\n",
        "                                                ])\n",
        "        self.encoder_conv_01 = nn.Sequential(*[\n",
        "                                                nn.Conv2d(in_channels=64,\n",
        "                                                          out_channels=64,\n",
        "                                                          kernel_size=3,\n",
        "                                                          padding=1),\n",
        "                                                nn.BatchNorm2d(64)\n",
        "                                                ])\n",
        "        self.encoder_conv_10 = nn.Sequential(*[\n",
        "                                                nn.Conv2d(in_channels=64,\n",
        "                                                          out_channels=128,\n",
        "                                                          kernel_size=3,\n",
        "                                                          padding=1),\n",
        "                                                nn.BatchNorm2d(128)\n",
        "                                                ])\n",
        "        self.encoder_conv_11 = nn.Sequential(*[\n",
        "                                                nn.Conv2d(in_channels=128,\n",
        "                                                          out_channels=128,\n",
        "                                                          kernel_size=3,\n",
        "                                                          padding=1),\n",
        "                                                nn.BatchNorm2d(128)\n",
        "                                                ])\n",
        "        self.encoder_conv_20 = nn.Sequential(*[\n",
        "                                                nn.Conv2d(in_channels=128,\n",
        "                                                          out_channels=256,\n",
        "                                                          kernel_size=3,\n",
        "                                                          padding=1),\n",
        "                                                nn.BatchNorm2d(256)\n",
        "                                                ])\n",
        "        self.encoder_conv_21 = nn.Sequential(*[\n",
        "                                                nn.Conv2d(in_channels=256,\n",
        "                                                          out_channels=256,\n",
        "                                                          kernel_size=3,\n",
        "                                                          padding=1),\n",
        "                                                nn.BatchNorm2d(256)\n",
        "                                                ])\n",
        "        self.encoder_conv_22 = nn.Sequential(*[\n",
        "                                                nn.Conv2d(in_channels=256,\n",
        "                                                          out_channels=256,\n",
        "                                                          kernel_size=3,\n",
        "                                                          padding=1),\n",
        "                                                nn.BatchNorm2d(256)\n",
        "                                                ])\n",
        "        self.encoder_conv_30 = nn.Sequential(*[\n",
        "                                                nn.Conv2d(in_channels=256,\n",
        "                                                          out_channels=512,\n",
        "                                                          kernel_size=3,\n",
        "                                                          padding=1),\n",
        "                                                nn.BatchNorm2d(512)\n",
        "                                                ])\n",
        "        self.encoder_conv_31 = nn.Sequential(*[\n",
        "                                                nn.Conv2d(in_channels=512,\n",
        "                                                          out_channels=512,\n",
        "                                                          kernel_size=3,\n",
        "                                                          padding=1),\n",
        "                                                nn.BatchNorm2d(512)\n",
        "                                                ])\n",
        "        self.encoder_conv_32 = nn.Sequential(*[\n",
        "                                                nn.Conv2d(in_channels=512,\n",
        "                                                          out_channels=512,\n",
        "                                                          kernel_size=3,\n",
        "                                                          padding=1),\n",
        "                                                nn.BatchNorm2d(512)\n",
        "                                                ])\n",
        "        self.encoder_conv_40 = nn.Sequential(*[\n",
        "                                                nn.Conv2d(in_channels=512,\n",
        "                                                          out_channels=512,\n",
        "                                                          kernel_size=3,\n",
        "                                                          padding=1),\n",
        "                                                nn.BatchNorm2d(512)\n",
        "                                                ])\n",
        "        self.encoder_conv_41 = nn.Sequential(*[\n",
        "                                                nn.Conv2d(in_channels=512,\n",
        "                                                          out_channels=512,\n",
        "                                                          kernel_size=3,\n",
        "                                                          padding=1),\n",
        "                                                nn.BatchNorm2d(512)\n",
        "                                                ])\n",
        "        self.encoder_conv_42 = nn.Sequential(*[\n",
        "                                                nn.Conv2d(in_channels=512,\n",
        "                                                          out_channels=512,\n",
        "                                                          kernel_size=3,\n",
        "                                                          padding=1),\n",
        "                                                nn.BatchNorm2d(512)\n",
        "                                                ])\n",
        "\n",
        "        self.init_vgg_weigts()\n",
        "\n",
        "        # Decoder layers\n",
        "\n",
        "        self.decoder_convtr_42 = nn.Sequential(*[\n",
        "                                                nn.ConvTranspose2d(in_channels=512,\n",
        "                                                                   out_channels=512,\n",
        "                                                                   kernel_size=3,\n",
        "                                                                   padding=1),\n",
        "                                                nn.BatchNorm2d(512)\n",
        "                                               ])\n",
        "        self.decoder_convtr_41 = nn.Sequential(*[\n",
        "                                                nn.ConvTranspose2d(in_channels=512,\n",
        "                                                                   out_channels=512,\n",
        "                                                                   kernel_size=3,\n",
        "                                                                   padding=1),\n",
        "                                                nn.BatchNorm2d(512)\n",
        "                                               ])\n",
        "        self.decoder_convtr_40 = nn.Sequential(*[\n",
        "                                                nn.ConvTranspose2d(in_channels=512,\n",
        "                                                                   out_channels=512,\n",
        "                                                                   kernel_size=3,\n",
        "                                                                   padding=1),\n",
        "                                                nn.BatchNorm2d(512)\n",
        "                                               ])\n",
        "        self.decoder_convtr_32 = nn.Sequential(*[\n",
        "                                                nn.ConvTranspose2d(in_channels=512,\n",
        "                                                                   out_channels=512,\n",
        "                                                                   kernel_size=3,\n",
        "                                                                   padding=1),\n",
        "                                                nn.BatchNorm2d(512)\n",
        "                                               ])\n",
        "        self.decoder_convtr_31 = nn.Sequential(*[\n",
        "                                                nn.ConvTranspose2d(in_channels=512,\n",
        "                                                                   out_channels=512,\n",
        "                                                                   kernel_size=3,\n",
        "                                                                   padding=1),\n",
        "                                                nn.BatchNorm2d(512)\n",
        "                                               ])\n",
        "        self.decoder_convtr_30 = nn.Sequential(*[\n",
        "                                                nn.ConvTranspose2d(in_channels=512,\n",
        "                                                                   out_channels=256,\n",
        "                                                                   kernel_size=3,\n",
        "                                                                   padding=1),\n",
        "                                                nn.BatchNorm2d(256)\n",
        "                                               ])\n",
        "        self.decoder_convtr_22 = nn.Sequential(*[\n",
        "                                                nn.ConvTranspose2d(in_channels=256,\n",
        "                                                                   out_channels=256,\n",
        "                                                                   kernel_size=3,\n",
        "                                                                   padding=1),\n",
        "                                                nn.BatchNorm2d(256)\n",
        "                                               ])\n",
        "        self.decoder_convtr_21 = nn.Sequential(*[\n",
        "                                                nn.ConvTranspose2d(in_channels=256,\n",
        "                                                                   out_channels=256,\n",
        "                                                                   kernel_size=3,\n",
        "                                                                   padding=1),\n",
        "                                                nn.BatchNorm2d(256)\n",
        "                                               ])\n",
        "        self.decoder_convtr_20 = nn.Sequential(*[\n",
        "                                                nn.ConvTranspose2d(in_channels=256,\n",
        "                                                                   out_channels=128,\n",
        "                                                                   kernel_size=3,\n",
        "                                                                   padding=1),\n",
        "                                                nn.BatchNorm2d(128)\n",
        "                                               ])\n",
        "        self.decoder_convtr_11 = nn.Sequential(*[\n",
        "                                                nn.ConvTranspose2d(in_channels=128,\n",
        "                                                                   out_channels=128,\n",
        "                                                                   kernel_size=3,\n",
        "                                                                   padding=1),\n",
        "                                                nn.BatchNorm2d(128)\n",
        "                                               ])\n",
        "        self.decoder_convtr_10 = nn.Sequential(*[\n",
        "                                                nn.ConvTranspose2d(in_channels=128,\n",
        "                                                                   out_channels=64,\n",
        "                                                                   kernel_size=3,\n",
        "                                                                   padding=1),\n",
        "                                                nn.BatchNorm2d(64)\n",
        "                                               ])\n",
        "        self.decoder_convtr_01 = nn.Sequential(*[\n",
        "                                                nn.ConvTranspose2d(in_channels=64,\n",
        "                                                                   out_channels=64,\n",
        "                                                                   kernel_size=3,\n",
        "                                                                   padding=1),\n",
        "                                                nn.BatchNorm2d(64)\n",
        "                                               ])\n",
        "        self.decoder_convtr_00 = nn.Sequential(*[\n",
        "                                                nn.ConvTranspose2d(in_channels=64,\n",
        "                                                                   out_channels=self.output_channels,\n",
        "                                                                   kernel_size=3,\n",
        "                                                                   padding=1)\n",
        "                                               ])\n",
        "\n",
        "\n",
        "    def forward(self, input_img):\n",
        "        \"\"\"\n",
        "        Forward pass `input_img` through the network\n",
        "        \"\"\"\n",
        "\n",
        "        # Encoder\n",
        "\n",
        "        # Encoder Stage - 1\n",
        "        dim_0 = input_img.size()\n",
        "        x_00 = F.relu(self.encoder_conv_00(input_img))\n",
        "        x_01 = F.relu(self.encoder_conv_01(x_00))\n",
        "        x_0, indices_0 = F.max_pool2d(x_01, kernel_size=2, stride=2, return_indices=True)\n",
        "\n",
        "        # Encoder Stage - 2\n",
        "        dim_1 = x_0.size()\n",
        "        x_10 = F.relu(self.encoder_conv_10(x_0))\n",
        "        x_11 = F.relu(self.encoder_conv_11(x_10))\n",
        "        x_1, indices_1 = F.max_pool2d(x_11, kernel_size=2, stride=2, return_indices=True)\n",
        "\n",
        "        # Encoder Stage - 3\n",
        "        dim_2 = x_1.size()\n",
        "        x_20 = F.relu(self.encoder_conv_20(x_1))\n",
        "        x_21 = F.relu(self.encoder_conv_21(x_20))\n",
        "        x_22 = F.relu(self.encoder_conv_22(x_21))\n",
        "        x_2, indices_2 = F.max_pool2d(x_22, kernel_size=2, stride=2, return_indices=True)\n",
        "\n",
        "        # Encoder Stage - 4\n",
        "        dim_3 = x_2.size()\n",
        "        x_30 = F.relu(self.encoder_conv_30(x_2))\n",
        "        x_31 = F.relu(self.encoder_conv_31(x_30))\n",
        "        x_32 = F.relu(self.encoder_conv_32(x_31))\n",
        "        x_3, indices_3 = F.max_pool2d(x_32, kernel_size=2, stride=2, return_indices=True)\n",
        "\n",
        "        # Encoder Stage - 5\n",
        "        dim_4 = x_3.size()\n",
        "        x_40 = F.relu(self.encoder_conv_40(x_3))\n",
        "        x_41 = F.relu(self.encoder_conv_41(x_40))\n",
        "        x_42 = F.relu(self.encoder_conv_42(x_41))\n",
        "        x_4, indices_4 = F.max_pool2d(x_42, kernel_size=2, stride=2, return_indices=True)\n",
        "\n",
        "        # Decoder\n",
        "\n",
        "        dim_d = x_4.size()\n",
        "\n",
        "        # Decoder Stage - 5\n",
        "        x_4d = F.max_unpool2d(x_4, indices_4, kernel_size=2, stride=2, output_size=dim_4)\n",
        "        x_42d = F.relu(self.decoder_convtr_42(x_4d))\n",
        "        x_41d = F.relu(self.decoder_convtr_41(x_42d))\n",
        "        x_40d = F.relu(self.decoder_convtr_40(x_41d))\n",
        "        dim_4d = x_40d.size()\n",
        "\n",
        "        # Decoder Stage - 4\n",
        "        x_3d = F.max_unpool2d(x_40d, indices_3, kernel_size=2, stride=2, output_size=dim_3)\n",
        "        x_32d = F.relu(self.decoder_convtr_32(x_3d))\n",
        "        x_31d = F.relu(self.decoder_convtr_31(x_32d))\n",
        "        x_30d = F.relu(self.decoder_convtr_30(x_31d))\n",
        "        dim_3d = x_30d.size()\n",
        "\n",
        "        # Decoder Stage - 3\n",
        "        x_2d = F.max_unpool2d(x_30d, indices_2, kernel_size=2, stride=2, output_size=dim_2)\n",
        "        x_22d = F.relu(self.decoder_convtr_22(x_2d))\n",
        "        x_21d = F.relu(self.decoder_convtr_21(x_22d))\n",
        "        x_20d = F.relu(self.decoder_convtr_20(x_21d))\n",
        "        dim_2d = x_20d.size()\n",
        "\n",
        "        # Decoder Stage - 2\n",
        "        x_1d = F.max_unpool2d(x_20d, indices_1, kernel_size=2, stride=2, output_size=dim_1)\n",
        "        x_11d = F.relu(self.decoder_convtr_11(x_1d))\n",
        "        x_10d = F.relu(self.decoder_convtr_10(x_11d))\n",
        "        dim_1d = x_10d.size()\n",
        "\n",
        "        # Decoder Stage - 1\n",
        "        x_0d = F.max_unpool2d(x_10d, indices_0, kernel_size=2, stride=2, output_size=dim_0)\n",
        "        x_01d = F.relu(self.decoder_convtr_01(x_0d))\n",
        "        x_00d = self.decoder_convtr_00(x_01d)\n",
        "        dim_0d = x_00d.size()\n",
        "\n",
        "        x_softmax = F.softmax(x_00d, dim=1)\n",
        "\n",
        "\n",
        "        if DEBUG:\n",
        "            print(\"dim_0: {}\".format(dim_0))\n",
        "            print(\"dim_1: {}\".format(dim_1))\n",
        "            print(\"dim_2: {}\".format(dim_2))\n",
        "            print(\"dim_3: {}\".format(dim_3))\n",
        "            print(\"dim_4: {}\".format(dim_4))\n",
        "\n",
        "            print(\"dim_d: {}\".format(dim_d))\n",
        "            print(\"dim_4d: {}\".format(dim_4d))\n",
        "            print(\"dim_3d: {}\".format(dim_3d))\n",
        "            print(\"dim_2d: {}\".format(dim_2d))\n",
        "            print(\"dim_1d: {}\".format(dim_1d))\n",
        "            print(\"dim_0d: {}\".format(dim_0d))\n",
        "\n",
        "\n",
        "        return x_00d, x_softmax\n",
        "\n",
        "\n",
        "    def init_vgg_weigts(self):\n",
        "        assert self.encoder_conv_00[0].weight.size() == self.vgg16.features[0].weight.size()\n",
        "        self.encoder_conv_00[0].weight.data = self.vgg16.features[0].weight.data\n",
        "        assert self.encoder_conv_00[0].bias.size() == self.vgg16.features[0].bias.size()\n",
        "        self.encoder_conv_00[0].bias.data = self.vgg16.features[0].bias.data\n",
        "\n",
        "        assert self.encoder_conv_01[0].weight.size() == self.vgg16.features[2].weight.size()\n",
        "        self.encoder_conv_01[0].weight.data = self.vgg16.features[2].weight.data\n",
        "        assert self.encoder_conv_01[0].bias.size() == self.vgg16.features[2].bias.size()\n",
        "        self.encoder_conv_01[0].bias.data = self.vgg16.features[2].bias.data\n",
        "\n",
        "        assert self.encoder_conv_10[0].weight.size() == self.vgg16.features[5].weight.size()\n",
        "        self.encoder_conv_10[0].weight.data = self.vgg16.features[5].weight.data\n",
        "        assert self.encoder_conv_10[0].bias.size() == self.vgg16.features[5].bias.size()\n",
        "        self.encoder_conv_10[0].bias.data = self.vgg16.features[5].bias.data\n",
        "\n",
        "        assert self.encoder_conv_11[0].weight.size() == self.vgg16.features[7].weight.size()\n",
        "        self.encoder_conv_11[0].weight.data = self.vgg16.features[7].weight.data\n",
        "        assert self.encoder_conv_11[0].bias.size() == self.vgg16.features[7].bias.size()\n",
        "        self.encoder_conv_11[0].bias.data = self.vgg16.features[7].bias.data\n",
        "\n",
        "        assert self.encoder_conv_20[0].weight.size() == self.vgg16.features[10].weight.size()\n",
        "        self.encoder_conv_20[0].weight.data = self.vgg16.features[10].weight.data\n",
        "        assert self.encoder_conv_20[0].bias.size() == self.vgg16.features[10].bias.size()\n",
        "        self.encoder_conv_20[0].bias.data = self.vgg16.features[10].bias.data\n",
        "\n",
        "        assert self.encoder_conv_21[0].weight.size() == self.vgg16.features[12].weight.size()\n",
        "        self.encoder_conv_21[0].weight.data = self.vgg16.features[12].weight.data\n",
        "        assert self.encoder_conv_21[0].bias.size() == self.vgg16.features[12].bias.size()\n",
        "        self.encoder_conv_21[0].bias.data = self.vgg16.features[12].bias.data\n",
        "\n",
        "        assert self.encoder_conv_22[0].weight.size() == self.vgg16.features[14].weight.size()\n",
        "        self.encoder_conv_22[0].weight.data = self.vgg16.features[14].weight.data\n",
        "        assert self.encoder_conv_22[0].bias.size() == self.vgg16.features[14].bias.size()\n",
        "        self.encoder_conv_22[0].bias.data = self.vgg16.features[14].bias.data\n",
        "\n",
        "        assert self.encoder_conv_30[0].weight.size() == self.vgg16.features[17].weight.size()\n",
        "        self.encoder_conv_30[0].weight.data = self.vgg16.features[17].weight.data\n",
        "        assert self.encoder_conv_30[0].bias.size() == self.vgg16.features[17].bias.size()\n",
        "        self.encoder_conv_30[0].bias.data = self.vgg16.features[17].bias.data\n",
        "\n",
        "        assert self.encoder_conv_31[0].weight.size() == self.vgg16.features[19].weight.size()\n",
        "        self.encoder_conv_31[0].weight.data = self.vgg16.features[19].weight.data\n",
        "        assert self.encoder_conv_31[0].bias.size() == self.vgg16.features[19].bias.size()\n",
        "        self.encoder_conv_31[0].bias.data = self.vgg16.features[19].bias.data\n",
        "\n",
        "        assert self.encoder_conv_32[0].weight.size() == self.vgg16.features[21].weight.size()\n",
        "        self.encoder_conv_32[0].weight.data = self.vgg16.features[21].weight.data\n",
        "        assert self.encoder_conv_32[0].bias.size() == self.vgg16.features[21].bias.size()\n",
        "        self.encoder_conv_32[0].bias.data = self.vgg16.features[21].bias.data\n",
        "\n",
        "        assert self.encoder_conv_40[0].weight.size() == self.vgg16.features[24].weight.size()\n",
        "        self.encoder_conv_40[0].weight.data = self.vgg16.features[24].weight.data\n",
        "        assert self.encoder_conv_40[0].bias.size() == self.vgg16.features[24].bias.size()\n",
        "        self.encoder_conv_40[0].bias.data = self.vgg16.features[24].bias.data\n",
        "\n",
        "        assert self.encoder_conv_41[0].weight.size() == self.vgg16.features[26].weight.size()\n",
        "        self.encoder_conv_41[0].weight.data = self.vgg16.features[26].weight.data\n",
        "        assert self.encoder_conv_41[0].bias.size() == self.vgg16.features[26].bias.size()\n",
        "        self.encoder_conv_41[0].bias.data = self.vgg16.features[26].bias.data\n",
        "\n",
        "        assert self.encoder_conv_42[0].weight.size() == self.vgg16.features[28].weight.size()\n",
        "        self.encoder_conv_42[0].weight.data = self.vgg16.features[28].weight.data\n",
        "        assert self.encoder_conv_42[0].bias.size() == self.vgg16.features[28].bias.size()\n",
        "        self.encoder_conv_42[0].bias.data = self.vgg16.features[28].bias.data"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mo2mwYoouS7f"
      },
      "source": [
        "# Metrics"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3pRHbzptuSU-"
      },
      "source": [
        "\n",
        "def pixel_accuracy(output, mask):\n",
        "    with torch.no_grad():\n",
        "        output = torch.argmax(F.softmax(output, dim=1), dim=1)\n",
        "        correct = torch.eq(output, mask).int()\n",
        "        accuracy = float(correct.sum()) / float(correct.numel())\n",
        "    return accuracy\n",
        "\n",
        "def mIoU(pred_mask, mask, smooth=1e-10, n_classes=23):\n",
        "    with torch.no_grad():\n",
        "        pred_mask = F.softmax(pred_mask, dim=1)\n",
        "        pred_mask = torch.argmax(pred_mask, dim=1)\n",
        "        pred_mask = pred_mask.contiguous().view(-1)\n",
        "        mask = mask.contiguous().view(-1)\n",
        "\n",
        "        iou_per_class = []\n",
        "        for clas in range(0, n_classes): #loop per pixel class\n",
        "            true_class = pred_mask == clas\n",
        "            true_label = mask == clas\n",
        "\n",
        "            if true_label.long().sum().item() == 0: #no exist label in this loop\n",
        "                iou_per_class.append(np.nan)\n",
        "            else:\n",
        "                intersect = torch.logical_and(true_class, true_label).sum().float().item()\n",
        "                union = torch.logical_or(true_class, true_label).sum().float().item()\n",
        "\n",
        "                iou = (intersect + smooth) / (union +smooth)\n",
        "                iou_per_class.append(iou)\n",
        "        return np.nanmean(iou_per_class)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YRDEZ8wyrvUE"
      },
      "source": [
        "# Apprentissage"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vksHm6Tc3WSs"
      },
      "source": [
        "Change the values\n",
        "\n",
        "```\n",
        "    NUM_INPUT_CHANNELS = 3\n",
        "    NUM_OUTPUT_CHANNELS = 2\n",
        "\n",
        "    NUM_EPOCHS = 100\n",
        "\n",
        "    LEARNING_RATE = 1e-3\n",
        "    MOMENTUM = 0.9\n",
        "    BATCH_SIZE = 8\n",
        "\n",
        "    save_dir=os.path.join(cdirectory,'model_weight_vege')\n",
        "\n",
        "    data_path =\n",
        "    \n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zy0Quk9I3vkx"
      },
      "source": [
        "NUM_INPUT_CHANNELS = 3\n",
        "NUM_OUTPUT_CHANNELS = 2\n",
        "\n",
        "NUM_EPOCHS = 100\n",
        "\n",
        "LEARNING_RATE = 1e-3\n",
        "MOMENTUM = 0.9\n",
        "BATCH_SIZE = 8\n",
        "\n",
        "\n",
        "save_dir=os.path.join(cdirectory,'model_weight_vege')\n",
        "\n",
        "data_path = "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3eUzBsdTrCn0"
      },
      "source": [
        "import csv\n",
        "import copy\n",
        "import time\n",
        "from tqdm import tqdm\n",
        "import torch\n",
        "import numpy as np\n",
        "import os \n",
        "from torchvision import transforms\n",
        "import torch.nn.functional as F\n",
        "def train_model(model, criterion, dataloaders, optimizer, metrics, bpath, num_epochs=3):\n",
        "    since = time.time()\n",
        "    best_model_wts = copy.deepcopy(model.state_dict())\n",
        "    best_loss = 1e10\n",
        "    # Use gpu if available\n",
        "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "    model.to(device)\n",
        "    count=0\n",
        "    # Initialize the log file for training and testing loss and metrics\n",
        "    fieldnames = ['epoch', 'Train_loss', 'Test_loss'] + \\\n",
        "        [f'Train_{m}' for m in metrics.keys()] + \\\n",
        "        [f'Test_{m}' for m in metrics.keys()]\n",
        "    with open(os.path.join(bpath, 'log.csv'), 'w', newline='') as csvfile:\n",
        "        writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
        "        writer.writeheader()\n",
        " \n",
        "    for epoch in range(1, num_epochs+1):\n",
        "        print('Epoch {}/{}'.format(epoch, num_epochs))\n",
        "        print('-' * 10)\n",
        "        # Each epoch has a training and validation phase\n",
        "        # Initialize batch summary\n",
        "        batchsummary = {a: [0] for a in fieldnames}\n",
        " \n",
        "        for phase in ['Train', 'Test']:\n",
        "          \n",
        "            if phase == 'Train':\n",
        "                model.train()  # Set model to training mode\n",
        "            else:\n",
        "                model.eval()   # Set model to evaluate mode\n",
        " \n",
        "            # Iterate over data.\n",
        "            for sample in tqdm(iter(dataloaders[phase])):\n",
        "                inputs = sample['image'].to(device)\n",
        "                masks = sample['mask'].to(device)\n",
        "                # zero the parameter gradients\n",
        "                optimizer.zero_grad()\n",
        " \n",
        "                # track history if only in train\n",
        "                with torch.set_grad_enabled(phase == 'Train'):\n",
        "                    # outputs = model(inputs)\n",
        "                    # loss = criterion(outputs['out'], masks)\n",
        "                    predicted_tensor, softmaxed_tensor = model(inputs)\n",
        "                    loss = criterion(predicted_tensor, masks.to(torch.long).squeeze(1))\n",
        "                    # predicted_tensor, softmaxed_tensor = model(inputs)\n",
        "\n",
        "                    # loss = criterion(predicted_tensor, masks)\n",
        "                    # y_pred = predicted_tensor.cpu().detach().numpy().ravel()\n",
        "                    # print('-----' * 5)\n",
        "                    for name, metric in metrics.items():\n",
        "                        if name == ' mIoU':\n",
        "                    #         # Use a classification threshold of 0.1\n",
        "                            batchsummary[f'{phase}_{name}'].append(\n",
        "                                metric(predicted_tensor, masks,n_classes=2))\n",
        "                        else:\n",
        "                            batchsummary[f'{phase}_{name}'].append(\n",
        "                                metric(predicted_tensor, masks))\n",
        " \n",
        "                    # backward + optimize only if in training phase\n",
        "                    if phase == 'Train':\n",
        "                        loss.backward()\n",
        "                        optimizer.step()\n",
        "\n",
        "            batchsummary['epoch'] = epoch\n",
        "            epoch_loss = loss\n",
        "            batchsummary[f'{phase}_loss'] = epoch_loss.item()\n",
        "            print('{} Loss: {:.4f}'.format(\n",
        "                phase, loss))\n",
        "        for field in fieldnames[3:]:\n",
        "            batchsummary[field] = np.mean(batchsummary[field])\n",
        "        print(batchsummary)\n",
        "        with open(os.path.join(bpath, 'log.csv'), 'a', newline='') as csvfile:\n",
        "            writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
        "            writer.writerow(batchsummary)\n",
        "            # deep copy the model\n",
        "            if phase == 'Test' and loss < best_loss:\n",
        "                best_loss = loss\n",
        "                best_model_wts = copy.deepcopy(model.state_dict())\n",
        "                torch.save(model.state_dict(), os.path.join(save_dir, \"model_best_vege.pth\"))\n",
        "\n",
        " \n",
        "    time_elapsed = time.time() - since\n",
        "    print('Training complete in {:.0f}m {:.0f}s'.format(\n",
        "        time_elapsed // 60, time_elapsed % 60))\n",
        "    print('Lowest Loss: {:4f}'.format(best_loss))\n",
        " \n",
        "    # load best model weights\n",
        "    model.load_state_dict(best_model_wts)\n",
        "\n",
        "    return model\n",
        "\n",
        "if __name__=='__main__':\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    if not os.path.isdir(save_dir):\n",
        "      os.makedirs(save_dir)\n",
        "\n",
        "\n",
        "    train_dataset = {\n",
        "        x: SegmentationDataset(data_path, \n",
        "                              \"train/image\", \n",
        "                              \"train/label\",\n",
        "                                seed=100,\n",
        "                                fraction=0.2,\n",
        "                                subset=x,\n",
        "                                transforms=transforms.Compose([transforms.ToTensor()]))  \n",
        "        for x in ['Train', 'Test']\n",
        "    }\n",
        "    train_dataloader = {\n",
        "        x: DataLoader(train_dataset[x],\n",
        "                        batch_size=8,\n",
        "                        shuffle=True,\n",
        "                        num_workers=2)\n",
        "        for x in ['Train', 'Test']\n",
        "    }\n",
        "    # Specify the loss function\n",
        "    # criterion = torch.nn.MSELoss(reduction='mean')\n",
        "    # Specify the optimizer with a lower learning rate\n",
        "    # optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n",
        "\n",
        "    # Specify the evaluation metrics\n",
        "    metrics = {'pixel_accuracy': pixel_accuracy, 'mIoU': mIoU}\n",
        "\n",
        "    model = SegNet(input_channels=NUM_INPUT_CHANNELS,\n",
        "                        output_channels=NUM_OUTPUT_CHANNELS)\n",
        "    # =======================================\n",
        "    # ========== if you already have a model\n",
        "    # model.load_state_dict(torch.load(Path(save_dir) / \"model_best_vege.pth\",map_location=torch.device('cpu')))\n",
        "    # model.train()\n",
        "\n",
        "    criterion = torch.nn.CrossEntropyLoss()\n",
        "    optimizer = torch.optim.SGD(model.parameters(), lr=LEARNING_RATE)    \n",
        "\n",
        "    model=train_model(model, criterion, train_dataloader, optimizer, metrics, save_dir, NUM_EPOCHS)\n",
        "    torch.save(model.state_dict(), os.path.join(save_dir, \"model_best.pth\"))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xAcISdJk10DD"
      },
      "source": [
        "# Inference"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y03RF9hC1sbi"
      },
      "source": [
        "from pathlib import Path\n",
        "model = SegNet(input_channels=3,\n",
        "                        output_channels=2)\n",
        "\n",
        "model.load_state_dict(torch.load(Path(save_dir) / \"model_best_vege.pth\",map_location=torch.device('cpu')))\n",
        "model.eval()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Huh-Ebz714pN"
      },
      "source": [
        "from torchvision.utils import make_grid as makeg\n",
        "gr=makeg(masked,nrow=8)\n",
        "grid_img=makeg(d['image'])\n",
        "grid_gt=makeg(d['mask'])\n",
        "\n",
        "plt.figure(figsize=(45,45))\n",
        "plt.imshow(grid_img.permute(1, 2, 0))\n",
        "plt.figure(figsize=(45,45))\n",
        "plt.imshow(gr.permute(1, 2, 0)*255)\n",
        "plt.figure(figsize=(45,45))\n",
        "plt.imshow(grid_gt.permute(1, 2, 0)*255)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}